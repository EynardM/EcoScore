{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement d'un modèle d'embeddings sur toutes les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des différents packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 21:00:49.869341: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-15 21:00:49.876782: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 21:00:49.990997: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 21:00:49.991080: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 21:00:49.991145: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 21:00:50.016417: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 21:00:50.018122: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 21:00:52.170725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review  label\n",
      "0     A total disappointment. Their 'ecological appr...      0\n",
      "1     An ecological pretension devoid of meaning. Th...      0\n",
      "2     Disappointing. Their alleged ecological consci...      0\n",
      "3     A restaurant that boasts of being ecological b...      0\n",
      "4     Ecological facades. Their discourse on ecology...      0\n",
      "...                                                 ...    ...\n",
      "1145  While not achieving top-tier performance in wa...      3\n",
      "1146  As someone who dined at this venue, the waste ...      3\n",
      "1147  In my experience at this joint, the waste mana...      3\n",
      "1148  First time eating here and it lived up to all ...      4\n",
      "1149  I absolutely love this chain. The salads are a...      2\n",
      "\n",
      "[2054 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Model import\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load your dataset (assuming you have a CSV file)\n",
    "\n",
    "df_adverse = pd.read_csv(\"Data/adverse_reviews.csv\")\n",
    "df_adverse = df_adverse[df_adverse[\"rate\"] != -1]\n",
    "\n",
    "\n",
    "df_climate = pd.read_csv(\"Data/climate_reviews.csv\")\n",
    "df_climate = df_climate[df_climate[\"rate\"] != -1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_governance = pd.read_csv(\"Data/governance_reviews.csv\")\n",
    "df_governance = df_governance[df_governance[\"rate\"] != -1]\n",
    "\n",
    "\n",
    "\n",
    "df_organic = pd.read_csv(\"Data/organic_reviews.csv\")\n",
    "df_organic = df_organic[df_organic[\"rate\"] != -1]\n",
    "\n",
    "df_social = pd.read_csv(\"Data/social_reviews.csv\")\n",
    "df_social = df_social[df_social[\"rate\"] != -1]\n",
    "\n",
    "\n",
    "df_waste = pd.read_csv(\"Data/waste_reviews.csv\")\n",
    "df_waste = df_waste[df_waste[\"rate\"] != -1]\n",
    "\n",
    "df = pd.read_csv(\"GptData/everything.csv\")\n",
    "\n",
    "df_all = pd.concat([df_adverse,df_climate,df_governance,df_organic,df_social,df_waste],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "# Change the colone name\n",
    "\n",
    "df_all['label'] = df_all[\"rate\"]\n",
    "df_all = df_all.drop(\"rate\",axis=1)\n",
    "\n",
    "df_all = pd.concat([df, df_all])\n",
    "print(df_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données d'entraînement, de test et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = train_test_split(df_all, test_size=0.2, random_state=42)\n",
    "test_data , val_data = train_test_split(test_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokénisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1643, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "max_words = 10000  # Choose the maximum number of words in your vocabulary\n",
    "max_len = 100  # Choose the maximum length of your sequences\n",
    "\n",
    "# Tokenizer init \n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['review'])\n",
    "\n",
    "\n",
    "# Encode the labels\n",
    "# we can take the data that have already been tokenized\n",
    "\n",
    "\n",
    "# Applying the tokenizer to the reviews with a padding\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['review']), maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['review']), maxlen=max_len)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_data['review']), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data['label'])\n",
    "y_test = label_encoder.transform(test_data['label'])\n",
    "y_val = label_encoder.transform(val_data['label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 747462 (2.85 MB)\n",
      "Trainable params: 747462 (2.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_grade = Sequential()\n",
    "model_grade.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model_grade.add(LSTM(128))\n",
    "model_grade.add(Dropout(0.3))\n",
    "model_grade.add(Dense(64, activation='relu'))\n",
    "model_grade.add(Dropout(0.1))\n",
    "model_grade.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model_grade.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "model_grade.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 [==============================] - 5s 70ms/step - loss: 1.7385 - accuracy: 0.2818 - val_loss: 1.4651 - val_accuracy: 0.3415\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 1.1723 - accuracy: 0.5265 - val_loss: 0.9826 - val_accuracy: 0.6951\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 0.6311 - accuracy: 0.8089 - val_loss: 0.5743 - val_accuracy: 0.8140\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.2852 - accuracy: 0.9044 - val_loss: 0.5275 - val_accuracy: 0.8232\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.1977 - accuracy: 0.9355 - val_loss: 0.4731 - val_accuracy: 0.8262\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.1362 - accuracy: 0.9550 - val_loss: 0.5802 - val_accuracy: 0.8293\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 3s 66ms/step - loss: 0.1504 - accuracy: 0.9446 - val_loss: 0.4522 - val_accuracy: 0.8720\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 0.1065 - accuracy: 0.9641 - val_loss: 0.4483 - val_accuracy: 0.8506\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 4s 69ms/step - loss: 0.1241 - accuracy: 0.9568 - val_loss: 0.5415 - val_accuracy: 0.8476\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 3s 64ms/step - loss: 0.0877 - accuracy: 0.9671 - val_loss: 0.5105 - val_accuracy: 0.8628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1d1ffbae60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grade.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.4784 - accuracy: 0.9036 - 81ms/epoch - 27ms/step\n",
      "Accuracy new data 0.9036144614219666\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "[4 2 0 5]\n"
     ]
    }
   ],
   "source": [
    "loss , acc = model_grade.evaluate(X_val,y_val,verbose=2)\n",
    "print(\"Accuracy new data\",acc)\n",
    "\n",
    "\n",
    "new_texts = [\"Best managed McDonald's I've ever seen!,\",\"Yo, socially, this joint is kinda in the middle. Moves and inclusivity are cool, like hosting a regular hangout. A chill experience, nothing too wild.\",\"A total disappointment. Their 'ecological approach' was a facade, the omnipresent plastic being blatant proof.\",\"Discovering the treasures within these walls reveals a masterpiece that surpasses expectations. The chef, a virtuoso in directing the kitchen, conducts an orchestra of flavors with finesse. It's not merely dining; it's an experience that transcends the realms of artistry.\"]\n",
    "new_sequences = pad_sequences(tokenizer.texts_to_sequences(new_texts), maxlen=max_len)\n",
    "\n",
    "predictions = model_grade.predict(new_sequences)\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
