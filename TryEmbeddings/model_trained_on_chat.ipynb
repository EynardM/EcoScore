{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement d'un modèle d'embeddings sur les données de ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import de toutes les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:24:33.325690: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-15 17:24:33.335112: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 17:24:33.390998: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 17:24:33.391042: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 17:24:33.391062: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 17:24:33.409436: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 17:24:33.411215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 17:24:34.547352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                review  label\n",
      "0    A total disappointment. Their 'ecological appr...      0\n",
      "1    An ecological pretension devoid of meaning. Th...      0\n",
      "2    Disappointing. Their alleged ecological consci...      0\n",
      "3    A restaurant that boasts of being ecological b...      0\n",
      "4    Ecological facades. Their discourse on ecology...      0\n",
      "..                                                 ...    ...\n",
      "899  My recent visit to this place exposed a neutra...      3\n",
      "900  Based on my experience at this establishment, ...      3\n",
      "901  While not achieving top-tier performance in wa...      3\n",
      "902  As someone who dined at this venue, the waste ...      3\n",
      "903  In my experience at this joint, the waste mana...      3\n",
      "\n",
      "[904 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"GptData/everything.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données d'entraînement, de test et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tokénisation des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723, 100)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad the text data\n",
    "max_words = 10000  # Choose the maximum number of words in your vocabulary\n",
    "max_len = 100  # Choose the maximum length of your sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['review'])\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['review']), maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['review']), maxlen=max_len)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_data['review']), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage des labels (notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encode the labels\n",
    "# we can take the data that have already been tokenized\n",
    "label_encoder_grade = LabelEncoder()\n",
    "y_train = label_encoder_grade.fit_transform(train_data['label'])\n",
    "y_test = label_encoder_grade.transform(test_data['label'])\n",
    "y_val = label_encoder_grade.transform(val_data['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 747462 (2.85 MB)\n",
      "Trainable params: 747462 (2.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Finally the model part\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model_grade = Sequential()\n",
    "model_grade.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model_grade.add(LSTM(128))\n",
    "model_grade.add(Dropout(0.3))\n",
    "model_grade.add(Dense(64, activation='relu'))\n",
    "model_grade.add(Dropout(0.1))\n",
    "model_grade.add(Dense(len(label_encoder_grade.classes_), activation='softmax'))\n",
    "\n",
    "model_grade.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "model_grade.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 80ms/step - loss: 1.7841 - accuracy: 0.1923 - val_loss: 1.7766 - val_accuracy: 0.2847\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 1.7104 - accuracy: 0.4149 - val_loss: 1.6547 - val_accuracy: 0.3750\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 1.3558 - accuracy: 0.5961 - val_loss: 1.3702 - val_accuracy: 0.4792\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.8532 - accuracy: 0.7137 - val_loss: 0.9675 - val_accuracy: 0.5764\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 0.5287 - accuracy: 0.8479 - val_loss: 0.6708 - val_accuracy: 0.7639\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.2991 - accuracy: 0.9170 - val_loss: 0.4981 - val_accuracy: 0.8056\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.2105 - accuracy: 0.9433 - val_loss: 0.5946 - val_accuracy: 0.8056\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.1463 - accuracy: 0.9585 - val_loss: 0.5179 - val_accuracy: 0.7986\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.1030 - accuracy: 0.9696 - val_loss: 0.5034 - val_accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.0702 - accuracy: 0.9737 - val_loss: 0.4719 - val_accuracy: 0.8403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd554d68100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grade.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 1.1226 - accuracy: 0.7297 - 43ms/epoch - 21ms/step\n",
      "Accuracy new data 0.7297297120094299\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "[0 2 0 5]\n"
     ]
    }
   ],
   "source": [
    "loss , acc = model_grade.evaluate(X_val,y_val,verbose=2)\n",
    "print(\"Accuracy new data\",acc)\n",
    "\n",
    "new_texts = [\"Best managed McDonald's I've ever seen!,\",\"Yo, socially, this joint is kinda in the middle. Moves and inclusivity are cool, like hosting a regular hangout. A chill experience, nothing too wild.\",\"A total disappointment. Their 'ecological approach' was a facade, the omnipresent plastic being blatant proof.\",\"Discovering the treasures within these walls reveals a masterpiece that surpasses expectations. The chef, a virtuoso in directing the kitchen, conducts an orchestra of flavors with finesse. It's not merely dining; it's an experience that transcends the realms of artistry.\"]\n",
    "new_sequences = pad_sequences(tokenizer.texts_to_sequences(new_texts), maxlen=max_len)\n",
    "\n",
    "predictions = model_grade.predict(new_sequences)\n",
    "predicted_labels = label_encoder_grade.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
